{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(name1,name2):\n",
    "    train = pd.read_csv(f\"datasets/{name1}.txt\", delim_whitespace=True, header=None)\n",
    "    X_train = train.iloc[:, :-1].values  # Features\n",
    "    y_train = train.iloc[:, -1].values    # Target\n",
    "\n",
    "    test = pd.read_csv(f\"datasets/{name2}.txt\", delim_whitespace=True, header=None)\n",
    "    X_test = test.iloc[:, :-1].values  # Features\n",
    "    y_test = test.iloc[:, -1].values    # Target\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = []\n",
    "        # Calculate mean and standard deviation for each class / feature\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.parameters.append([(np.mean(X_c[:, i]), np.std(X_c[:, i])) for i in range(X_c.shape[1])])\n",
    "        \n",
    "\n",
    "    #NormalPDF probability of X P(x)\n",
    "    def _calculate_probability(self, x, mean, stdev):\n",
    "        exponent = np.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "        return (1 / (np.sqrt(2 * np.pi) * stdev)) * exponent\n",
    "\n",
    "    #Looping through dataset\n",
    "    def _calculate_class_probabilities(self, x):\n",
    "        probabilities = {}\n",
    "        for i, c in enumerate(self.classes):\n",
    "            probabilities[c] = 1\n",
    "            for j, param in enumerate(self.parameters[i]):\n",
    "                mean, stdev = param\n",
    "                probabilities[c] *= self._calculate_probability(x[j], mean, stdev)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        #foreach class find most likely probability datapoint is a given class\n",
    "        for x in X:\n",
    "            probabilities = self._calculate_class_probabilities(x)\n",
    "            best_class = None\n",
    "            best_prob = -1\n",
    "            for c, prob in probabilities.items():\n",
    "                if best_class is None or prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_class = c\n",
    "            predictions.append(best_class)\n",
    "        return predictions\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    return correct / total\n",
    "\n",
    "def true_positives(y_true, y_pred, positive_label):\n",
    "    return np.sum((y_true == positive_label) & (y_pred == positive_label))\n",
    "\n",
    "def false_positives(y_true, y_pred, positive_label):\n",
    "    return np.sum((y_true != positive_label) & (y_pred == positive_label))\n",
    "\n",
    "def true_negatives(y_true, y_pred, negative_label):\n",
    "    return np.sum((y_true == negative_label) & (y_pred == negative_label))\n",
    "\n",
    "def false_negatives(y_true, y_pred, negative_label):\n",
    "    return np.sum((y_true != negative_label) & (y_pred != negative_label))\n",
    "\n",
    "def precision(y_true, y_pred, positive_label):\n",
    "    tp = true_positives(y_true, y_pred, positive_label)\n",
    "    fp = false_positives(y_true, y_pred, positive_label)\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(y_true, y_pred, positive_label):\n",
    "    tp = true_positives(y_true, y_pred, positive_label)\n",
    "    fn = false_negatives(y_true, y_pred, positive_label)\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def print_all(y_test,y_pred):\n",
    "    print(f'accurancy: {accuracy(y_test, y_pred)}')\n",
    "    print(f'true pos: {true_positives(y_test, y_pred, positive_label=1)}')  # positive class label is 1\n",
    "    print(f'false pos: {false_positives(y_test, y_pred, positive_label=1)}')\n",
    "    print(f'true neg: {true_negatives(y_test, y_pred, negative_label=-1)}')  # negative class label is -1\n",
    "    print(f'false neg: {false_negatives(y_test, y_pred, negative_label=-1)}')\n",
    "    print(f'precision: {precision(y_test, y_pred, positive_label=1)}')\n",
    "    print(f'recall: {recall(y_test, y_pred, positive_label=1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy: 0.94\n",
      "true pos: 16\n",
      "false pos: 3\n",
      "true neg: 31\n",
      "false neg: 16\n",
      "precision: 0.8421052631578947\n",
      "recall: 0.3404255319148936\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes()\n",
    "X_train, y_train, X_test, y_test = read_data(\"irisTraining\",\"irisTesting\")\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = np.array(nb.predict(X_test))\n",
    "print_all(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy: 0.75\n",
      "true pos: 2\n",
      "false pos: 1\n",
      "true neg: 1\n",
      "false neg: 2\n",
      "precision: 0.6666666666666666\n",
      "recall: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes()\n",
    "X_train, y_train, X_test, y_test = read_data(\"buyTraining\",\"buyTesting\")\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = np.array(nb.predict(X_test))\n",
    "print_all(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy: 0.92\n",
      "true pos: 16\n",
      "false pos: 4\n",
      "true neg: 30\n",
      "false neg: 16\n",
      "precision: 0.8\n",
      "recall: 0.34782608695652173\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes()\n",
    "X_train, y_train, X_test, y_test = read_data(\"irisPCTraining\",\"irisPCTesting\")\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = np.array(nb.predict(X_test))\n",
    "print_all(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy represents the number of correctly predicted classifications over the total number of classifications. (.94,.75,.92)\n",
    "\n",
    "true positive is classified positives which were positive,(16,2,16)\n",
    "\n",
    "false positive is classified positives which were negative,(3,1,4)\n",
    "\n",
    "true negative is classified negatives which were negative,(31,1,30)\n",
    "\n",
    "false negative is classified negatives which were positive.(16,2,16)\n",
    "\n",
    "Precision is looking at the accuracy of positive classifications, (.842,.66,80)\n",
    "\n",
    "and Recall is the accuracy of all actual (base) positive classes, and their state after classification.(.34,.66,.348)\n",
    "\n",
    "--\n",
    "\n",
    "Our Model has a higher precision, and lower recall, meaning that it is prioritizing accurate positive predictions, over coverage of all positive instances. In the context of this dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
